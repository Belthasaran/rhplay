# Nostr Client Integration Plan

## 1. Objectives
- Provide a resilient Nostr client stack that can publish and subscribe to RHTools events (profiles, keypairs, trust declarations, ratings, future forum/chat objects).
- Support both foreground (Electron renderer) and optional background service operation so that events can be processed even when the main UI is closed.
- Introduce explicit Online vs Offline operating modes so the user controls when network connectivity is used.
- Maintain a consistent follow list derived from trusted admin/master/admin-user keypairs and relevant subject keys, enabling automatic discovery of authoritative updates.
- Lay groundwork for advanced features: secure messaging, forum threads, user verification, and cross-device synchronization.

## 2. Functional Requirements
- **Outgoing events**: Persist, sign, queue, and broadcast Nostr events generated by the app (existing profile/keypair/rating publications, future declarations).
- **Incoming events**: Subscribe to relays, download events matching filters (follow list, app-specific kinds), validate signatures, deduplicate, and persist raw data in the appropriate `nostr_*` databases.
- **Offline resilience**: Continue relay synchronization in the background process with exponential backoff, relay health tracking, and catch-up logic when the foreground client reconnects.
- **Follow list management**: Automatically derive the follow list from:
  - All admin master/admin keypairs with Nostr public keys.
  - Subject keys of trust declarations.
  - User profile keys that published game ratings relevant to the library (extendable).
- **Security & privacy**: Keep secret keys in secure storage; decrypt only in the process requiring signing. Provide encryption primitives for DMs (future work).
- **Configurability**: Allow users to control relays, background service enablement, bandwidth constraints, Storage TTL, and logging.

## 3. Architecture Overview

```
+---------------------------+          +---------------------------+
| Electron Renderer (UI)   |  IPC/RPC |    Nostr Runtime Service  |
| - Publish requests       | <------> |  - Relay connections      |
| - Relay status display   |          |  - Event subscription     |
| - Feed presentation      |          |  - Outgoing queue flush   |
+---------------------------+          |  - Follow list refresh    |
             ^                         |  - Background scheduler   |
             |                         +-----------+---------------+
             |                                     |
             |                             +-------v-------+
             |                             | Database Tier |
             |                             | (SQLite files)|
             |                             +---------------+
             v
+---------------------------+
|  OnlineProfileManager     |
|  NostrLocalDBManager      |
+---------------------------+
```

### 3.1 Processes
- **Main Electron process**: Spawns/manages a dedicated Node.js worker (or Electron BrowserWindow with background scripts) called the *Nostr Runtime Service* (NRS). NRS persists across renderer reloads and can optionally continue when the UI is closed.
- **Renderer**: Sends commands to NRS via IPC channels (through `preload` bridge) and receives status updates (subscriptions, feed updates, errors).
- **Optional stand-alone daemon**: For headless operation, bundle NRS as a CLI entry point (`enode.sh nostr-daemon`), reusing shared libraries.

### 3.2 Data Flow
1. Renderer requests publication → IPC to NRS → NRS signs event (via `OnlineProfileManager`/`NostrLocalDBManager`) → event stored in `nostr_cache_out` → relay send.
2. NRS subscribes to configured relays → receives events → stores raw event in `nostr_cache_in` + archives → deduplicates → signals renderer via IPC for UI updates.
3. Follow list updates (see §4) trigger a resubscription cycle with new filters.
4. Operating mode transitions (`Offline` ⇄ `Online`) propagate from renderer → main process → NRS, which enables or suspends relay connectivity and queue flushing accordingly.

### 3.3 Operating Modes
- **Offline Mode** (default for first-time users):
  - No relay connections; outgoing events remain queued with status `pending`.
  - Incoming subscriptions paused; background tasks limited to local maintenance (e.g., follow list computation).
  - Renderer surfaces offline indicators and provides a call-to-action to enable Online Mode.
- **Online Mode** (requires explicit user consent):
  - NRS opens relay connections, processes outgoing queue, subscribes for incoming events, and optionally keeps running in the background.
  - Mode state persisted in `csettings` (e.g., `nostr_operating_mode`) and mirrored in renderer state.
  - Provide user controls to restrict Online Mode to foreground-only operation if desired.
- **Mode switching**:
  - Triggered via UI toggle with confirmation dialog summarizing network activity.
  - When switching to Online, reinitialize relay pool and flush pending events.
  - When switching to Offline, gracefully close connections and pause workers while keeping queued events intact.

### 3.4 Library Selection

| Library | Strengths | Limitations | Proposed Usage |
|---------|-----------|-------------|----------------|
| `nostr-tools` | Mature primitives, signing, event builders, utilities. | Low-level; manual relay handling. | **Core cryptography and basic client operations**. Already in use; keep for signing, event ID, nip-19 encoding. |
| `@nostr/gadgets` | High-level client toolkit, relay pooling, persistent subscriptions, advanced filtering, batching. | Newer; evaluate stability. | **Primary runtime** for relay management, worker orchestration, queue handling. Provides best path to support advanced features (DMs, groups). |
| `@nostr-dev-kit/ndk` | Comprehensive client framework, caching, background workers, many extensions (NIP-07/46). | Larger dependency, but battle-tested. | Integrate selectively (e.g., fallback to NDK for features Gadgets lacks). |
| `@nostrify/nostrify` | Utility functions, async iterators, minimal overhead. | Minimal feature set. | Supplementary for stream processing if needed. |
| `@nostr-connect/connect` | Relay connection manager for remote signing (NIP-46). | May be overkill now. | Reserve for remote signer integration (future high-security operations). |

**Recommendation**: Build NRS on top of `@nostr/gadgets` for relay orchestration + `nostr-tools` primitives. Keep NDK as optional plugin if we need robust connection pooling or relay scoring. Plan abstractions so either backend can be swapped.

## 4. Follow List Management
- **Source data**:
  - `admin_keypairs`, `profile_keypairs` tables for any Nostr keys (master/admin/user op).
  - `admindeclarations` subjects referencing Nostr public keys (parse `content_json`).
  - Explicit user-configured follows (future UI).
- **Processing**:
  - Periodic task (e.g., every 10 minutes or on data change) recomputes follow set.
  - Store in `nostr_follow_entries` table with metadata (type, source, last_verified).
  - NRS updates subscription filters (kinds 0, 3, 31001, 31106, etc.) accordingly.
- **Extensibility**: Provide weighting/priority for master/admin keys to ensure their events are never dropped.

## 5. Relay Management & Configuration
- **Relay catalog**:
  - Create `nostr_relays` table in `clientdata.db` (managed by `NostrLocalDBManager`) with fields: `relay_url`, `label`, `categories` (JSON array of tags), `priority`, `auth_required`, `read`, `write`, `added_by` (`system`, `user`, `admin-published`), `created_at`, `updated_at`.
  - Seed table with preload relays distributed with the client; allow updates via admin-signed metadata events.
- **Category preferences**:
  - Store user-selected relay categories in `csettings` entry `nostr_relay_categories` (array of tags). Defaults to curated categories (e.g., `trusted-core`, `ratings`, `profiles`).
  - During startup, NRS selects relays matching preferred categories plus any manually pinned relays.
- **Dynamic updates**:
  - Renderer UI allows users to add/remove relays, assign categories, and override read/write flags.
  - Admin-published relay lists ingested via Nostr events update the table with provenance metadata.
- **Health tracking**:
  - Maintain success/failure counters and last latency for each relay; expose via IPC for UI display.

## 6. Event Handling Strategy
- **Outgoing** (existing `nostr_cache_out`):
  - Implement queue worker in NRS reading pending rows, signing if necessary, pushing to relays.
  - Track per-event status (pending, sent, confirmed). Update `nostr_status` columns in relevant tables (keypairs, declarations, profiles).
- **Incoming** (`nostr_cache_in`, `nostr_store_*`, `nostr_archive_*`):
  - For each event: validate signature, store raw JSON, dedupe by event ID, index by kind, author, `d` tag.
  - Parse and route to domain-specific handlers (profiles, ratings, declarations). Update domain tables (`user_profiles`, `admindeclarations`, etc.) or queue for moderation review.
- **Retention policy**: use `keep_for` column to control TTL; archive older events to `nostr_archive_##` automatically.

## 7. Background Service Implementation
- **Process model**: Node worker launched via `child_process.fork()` from main process, with message channel for IPC, or use Electron's `BrowserWindow` running in headless mode for easier reuse of Node + DOM APIs.
- **Lifecycle**:
  - Auto-start with app (configurable). Provide controls in settings to pause/stop.
  - When Electron app closes, prompt user to keep service running; if accepted, detach to run as long-lived background process (Linux: systemd service entries optional; Windows: scheduled task). Document manual start via CLI.
- **Resource management**: ensure service respects rate limits, manages reconnections, and handles relay selection/preferences per user settings.
  - Defaults: total unprocessed incoming queue threshold (e.g., 5,000 events) after which intake pauses until backlog processed.
  - Message rate limiter: weighted average (default 100 units per 2 minutes, event unit = ceil(size/32KB)). If exceeded, pause intake for 60 seconds.
  - CPU usage guard: monitor worker CPU; if > configurable threshold (default 35%) over rolling window, back off subscriptions.
  - Outgoing throttle: limit to configurable events/minute (default 60), queue additional events for next interval.
  - Advanced options surfaced in settings; basic users rely on safe defaults.

## 8. Security Considerations
- Use `safeStorage` + Profile Guard encryption for secret keys. NRS requests secrets through `OnlineProfileManager`, decrypts in-memory only, zeroises after signing.
- Provide sandbox for NRS (no renderer access). Validate all IPC payloads.
- Implement optional Tor/proxy support (`socks-proxy-agent`).
- For DMs/E2EE: plan to implement NIP-04/NIP-44 using libs from `nostr-tools` or `@nostr/gadgets` (supports encryption helpers).
- Archived decrypted content: when storing offline copies of decrypted messages, encrypt payloads with dedicated AES-256 key cached per user and protected via Profile Guard (keyguard-managed key wrapping).

## 9. Implementation Phases
1. **Foundations**
   - Finalize `NostrLocalDBManager` APIs (relay config, queue operations).
   - Implement NRS scaffold (process startup, IPC contract, logging, configuration storage in `csettings`).
   - Integrate `@nostr/gadgets` with relay connection pooling; read/write events from caches.
   - Persist and expose the operating mode flag (`Offline`/`Online`) with default Offline and IPC hooks for toggling.
2. **Follow List & Subscription**
   - Create `nostr_follow_entries` table and sync task.
   - Build subscription filters (kinds and authors) and hook them to NRS.
3. **Incoming Event Routing**
   - Implement domain handlers (profiles, trust declarations, ratings) with validation pipeline.
   - Provide renderer notifications (e.g., new reviews available).
4. **Background Operation**
   - Add user controls, system tray indicator, CLI entry point.
   - Document how to run service headless.
5. **Advanced Features Roadmap**
   - DM encryption (NIP-04/44), forum announcements (kind 31100+), chat (kind 42), remote signer support, moderation queue UI.

## 10. IPC Contracts (Renderer ↔ NRS)
All commands travel through Electron's IPC (`ipcMain.handle`/`ipcRenderer.invoke`) with payloads serialized as plain objects. Events (streaming updates) use `ipcRenderer.on` from the renderer side. Core channels:

| Channel | Direction | Payload | Description |
|---------|-----------|---------|-------------|
| `nostr:nrs:init` | Renderer → Main | `{ modePreference?: "online" | "offline" }` | Renderer indicates current preference; main replies with current state, relays, limits. |
| `nostr:nrs:status` | Main → Renderer (event) | `{ mode, relays, queueStats, cpuLoad, backlog }` | Periodic status broadcast (every 5s or upon change). |
| `nostr:nrs:set-mode` | Renderer → Main | `{ mode: "online" | "offline", confirm?: boolean }` | Request mode change. Main prompts user if consent needed, then instructs NRS. Reply includes success flag and message. |
| `nostr:nrs:publish` | Renderer → Main | `{ eventUuid }` | Ask NRS to enqueue/flush specific event. Response contains updated status. |
| `nostr:nrs:queue:list` | Renderer → Main | `{ limit?, offset? }` | Fetch pending/sent queue entries for UI display. |
| `nostr:nrs:relays:list` | Renderer ↔ Main | Request: `{}`; Response: array of relay records with categories, health stats. |
| `nostr:nrs:relays:update` | Renderer → Main | `{ relayUrl, changes }` | Modify relay flags/categories; main persists to DB. |
| `nostr:nrs:relays:add` | Renderer → Main | `{ relayUrl, categories, read, write }` | Add manual relay. |
| `nostr:nrs:relays:remove` | Renderer → Main | `{ relayUrl }` | Remove user relay (system relays are flagged and require confirmation). |
| `nostr:nrs:follow:update` | Renderer → Main | `{ manualFollows }` | Update manual follow list; triggers NRS resubscription. |
| `nostr:nrs:event` | Main → Renderer (event) | `{ type: "incoming" | "outgoing" | "error", payload }` | Stream new events (for feed updates), errors, or queue status changes. |
| `nostr:nrs:limits:get` | Renderer → Main | `{}` | Retrieve current resource limits and defaults. |
| `nostr:nrs:limits:set` | Renderer → Main | `{ incomingBacklog?, messageRate?, cpuCap?, outgoingRate? }` | Update advanced throttles; persisted in `csettings`. |
| `nostr:nrs:shutdown` | Renderer → Main | `{ keepBackground?: boolean }` | Notify NRS of app exit; optionally keep background service alive. |

Errors returned as `{ success: false, errorCode, message }` with log correlation ID for diagnostics. Main process is responsible for routing calls to the NRS worker via message channels and returning replies to renderer.

## 11. Open Questions
- Multi-profile support: ensure per-profile identity separation while sharing runtime resources.
- Background consent UX: do we prompt again after major updates or rely on stored consent?

## 12. Next Steps
1. Review/approve this architecture and library selection.
2. Lock down IPC contracts between renderer and NRS.
3. Define database schema additions (follow table, relay config table, incoming event indices).
4. Schedule implementation milestones aligned with UI work (see companion UI plan).

